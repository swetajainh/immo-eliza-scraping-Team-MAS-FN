{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR CODE TRIAL \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Function to scrape URLs\n",
    "def scrape_urls(page_num):\n",
    "    base_url = f\"https://www.immoweb.be/en/search/house-and-apartment/for-sale?countries=BE&page={page_num}&orderBy=relevance\"\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    urls = []\n",
    "    for elem in soup.find_all(\"a\", attrs={\"class\": \"card__title-link\"}):\n",
    "        urls.append(elem.get('href'))\n",
    "\n",
    "    # Save URLs to file - full_list.txt (local storage)\n",
    "    with open(\"full_list.txt\", \"a\") as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "    return urls\n",
    "\n",
    "def thread_scraping():\n",
    "    full_list_url = []\n",
    "    num_pages = 2\n",
    "\n",
    "    # Create a list to store threads\n",
    "    threads = []\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    # Create and start threads\n",
    "    for i in range(1, num_pages + 1):\n",
    "        t = threading.Thread(target=lambda: full_list_url.extend(scrape_urls(i)))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait for all threads to complete and then join\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end_time = time.time()  # Stop timer\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total URLs scraped:\", len(full_list_url))\n",
    "    print(\"Total time:\", execution_time, \"seconds\")\n",
    "    return full_list_url\n",
    "\n",
    "thread_scraping()\n",
    "\n",
    "\n",
    "def reporting(str, i):\n",
    "    \"\"\"Reports on scraping progress\"\"\"\n",
    "    sys.stdout.write(str + ' %d\\r' %i)\n",
    "    sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "# Initialize counter for the counter function\n",
    "counters = 1\n",
    "def counter():\n",
    "    \"\"\"Creates a global counter for use in list comprehension\"\"\"\n",
    "    global counters\n",
    "    if counters < 1:\n",
    "        counters = 1\n",
    "    else:\n",
    "        counters += 1\n",
    "    return\n",
    "\n",
    "def scrape_house(url):\n",
    "    \"\"\"Scrapes all the info from a house listing\"\"\"\n",
    "\n",
    "    # Get the house listing and make a soup\n",
    "    try:\n",
    "        house_page = requests.get(url)\n",
    "        house_page = BeautifulSoup(house_page.text, 'html.parser')\n",
    "    # Return an empty dictionary if we can't parse the URL\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    # Get the hidden info from the java script\n",
    "    try:\n",
    "        regex = r\"window.classified = (\\{.*\\})\" # Only captures what's between brackets\n",
    "        script = house_page.find('div',attrs={\"id\":\"main-container\"}).script.text\n",
    "        script = re.findall(regex, script)\n",
    "        script = json.loads(script[0])\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    final_dictionary = {}\n",
    "    # URL\n",
    "    try:\n",
    "        final_dictionary['url'] = url\n",
    "    except:\n",
    "        final_dictionary['url'] = 'UNKNOWN'\n",
    "    # URL adding IMMO ID\n",
    "    try:\n",
    "        final_dictionary['id'] = script['id']\n",
    "    except:\n",
    "        final_dictionary['id'] = 'UNKNOWN'\n",
    "    # Region\n",
    "    try:\n",
    "        final_dictionary['region'] = script['property']['location']['region']\n",
    "    except:\n",
    "        final_dictionary['region'] = 'UNKNOWN'\n",
    "    #  # Region\n",
    "    try:\n",
    "        final_dictionary['region'] = script['property']['location']['region']\n",
    "    except:\n",
    "        final_dictionary['region'] = 'UNKNOWN'\n",
    "    # Province\n",
    "    try:\n",
    "        final_dictionary['province'] = script['property']['location']['province']\n",
    "    except:\n",
    "        final_dictionary['province'] = 'UNKNOWN'\n",
    "    # Locality\n",
    "    try:\n",
    "        final_dictionary['locality'] = script['property']['location']['locality']\n",
    "    except:\n",
    "        final_dictionary['locality'] = 'UNKNOWN'\n",
    "    # ZIP Code\n",
    "    try:\n",
    "        final_dictionary['zip_code'] = script['property']['location']['postalCode']\n",
    "    except:\n",
    "        final_dictionary['zip_code'] = 'UNKNOWN'\n",
    "    # Longitude\n",
    "    try:\n",
    "        final_dictionary['Longitude'] = script['property']['location']['longitude']\n",
    "    except:\n",
    "        final_dictionary['Longitude'] = 'UNKNOWN'\n",
    "    # Latitude\n",
    "    try:\n",
    "        final_dictionary['Latitude'] = script['property']['location']['latitude']\n",
    "    except:\n",
    "        final_dictionary['Latitude'] = 'UNKNOWN'\n",
    "    # Type of property\n",
    "    try:\n",
    "        final_dictionary['property_type'] = script['property']['type']\n",
    "    except:\n",
    "        final_dictionary['property_type'] = 'UNKNOWN'\n",
    "    # Subtype of property\n",
    "    try:\n",
    "        final_dictionary['property_subtype'] = script['property']['subtype']\n",
    "    except:\n",
    "        final_dictionary['property_subtype'] = 'UNKNOWN'\n",
    "    # Price\n",
    "    try:\n",
    "        final_dictionary['price'] = script['price']['mainValue']\n",
    "    except:\n",
    "        final_dictionary['price'] = 'UNKNOWN'\n",
    "    # Number of rooms\n",
    "    try:\n",
    "        final_dictionary['number_rooms'] = script['property']['bedroomCount']\n",
    "    except:\n",
    "        final_dictionary['number_rooms'] = 'UNKNOWN'\n",
    "    # Living area\n",
    "    try:\n",
    "        final_dictionary['living_area'] = script['property']['netHabitableSurface']\n",
    "    except:\n",
    "        final_dictionary['living_area'] = 'UNKNOWN'\n",
    "    # Fully equipped kitchen (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['kitchen'] = script['property']['kitchen']['type']\n",
    "    except:\n",
    "        final_dictionary['kitchen'] = 0\n",
    "    # Furnished (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['furnished'] = script['transaction']['sale']['isFurnished']\n",
    "    except:\n",
    "        final_dictionary['furnished'] = 'UNKNOWN'\n",
    "    # Open fire (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['fireplace'] = script['property']['fireplaceCount']\n",
    "    except:\n",
    "        final_dictionary['fireplace'] = 0\n",
    "    # Terrace (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['terrace'] = script['property']['hasTerrace']\n",
    "    except:\n",
    "        final_dictionary['terrace'] = 0\n",
    "    # If yes: Area\n",
    "    try:\n",
    "        final_dictionary['terrace_area'] = script['property']['terraceSurface']\n",
    "    except:\n",
    "        final_dictionary['terrace_area'] = 0\n",
    "    # Garden\n",
    "    try:\n",
    "        final_dictionary['garden'] = script['property']['hasGarden']\n",
    "    except:\n",
    "        final_dictionary['garden'] = 0\n",
    "    # If yes: Area\n",
    "    try:\n",
    "        final_dictionary['garden_area'] = script['property']['gardenSurface']\n",
    "    except:\n",
    "        final_dictionary['garden_area'] = 0\n",
    "    # Surface of the land\n",
    "    try:\n",
    "        final_dictionary['surface_land'] = script['property']['land']['surface']\n",
    "    except:\n",
    "        final_dictionary['surface_land'] = \"UNKNOWN\"\n",
    "    # Number of facades\n",
    "    try:\n",
    "        final_dictionary['number_facades'] = script['property']['building']['facadeCount']\n",
    "    except:\n",
    "        final_dictionary['number_facades'] = \"UNKNOWN\"\n",
    "    # Swimming pool (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['swimming_pool'] =  script['property']['hasSwimmingPool']\n",
    "    except:\n",
    "        final_dictionary['swimming_pool'] = 0\n",
    "    # State of the building (New, to be renovated, ...)\n",
    "    try:\n",
    "        final_dictionary['building_state'] = script['property']['building']['condition']\n",
    "    except:\n",
    "        final_dictionary['building_state'] = 'UNKNOWN'\n",
    "\n",
    "    return final_dictionary\n",
    "\n",
    "\n",
    "    return final_dictionary\n",
    "\n",
    "def create_dataframe():\n",
    "    \"\"\"Will scrape info from house pages and create a pandas DataFrame from the info we scrape\"\"\"\n",
    "    # Initialize list and fetch all URLs\n",
    "    houses_links = []\n",
    "    houses_links = thread_scraping()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Scraping individual pages...\")\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    # Scrape info from house pages concurrently\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [(executor.submit(scrape_house, url), counter(), reporting(\"Individual pages scraped:\", counters), time.sleep(.2)) for url in houses_links]\n",
    "        results =  [item[0].result() for item in futures]\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "    # Export our dataset to a csv\"\n",
    "    # Build path to file\n",
    "    # Select current working directory \n",
    "        cwd = Path.cwd()\n",
    "    df.to_csv(csv_path, index = True)\n",
    "\n",
    "    end_time = time.time()  # Stop timer\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(\"Scraping completed!                        \")\n",
    "    print(\"Total time spent scraping:\", execution_time, \"seconds\")\n",
    "    return df\n",
    "\n",
    "# Initialize counter for the counter function\n",
    "counters = 1\n",
    "\n",
    "# Build path to file\n",
    "# Selects current working directory\n",
    "cwd = Path.cwd()\n",
    "output_folder = (cwd / 'data_output').resolve() # Adjusted CSV file path and name\n",
    "csv_filename = \"house_apart_sale.csv\"\n",
    "csv_path = (output_folder / csv_filename).resolve()\n",
    "url_path = './full_list.txt'\n",
    "csv_path = (cwd / csv_path).resolve()\n",
    "url_path = (cwd / url_path).resolve()\n",
    "\n",
    "# Ensure the \"output\" folder exists\n",
    "output_folder = (cwd / 'data_output').resolve()\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset = create_dataframe()\n",
    "print(\"Original DataFrame:\")\n",
    "print(dataset)\n",
    "\n",
    "# Print unique values in the 'furnished' column before recoding\n",
    "print(\"Unique values in 'region' column before recoding:\")\n",
    "print(dataset['region'].unique())\n",
    "\n",
    "# Print 'furnished' column before recoding\n",
    "# print(\"\\n'furnished' column before recoding:\")\n",
    "# print(dataset['furnished'].head())\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "binary_columns = ['furnished', 'terrace', 'garden', 'swimming_pool']\n",
    "\n",
    "# Convert 'TRUE'/'FALSE' strings to 1/0 integers and handle empty values\n",
    "for column in binary_columns:\n",
    "    dataset[column] = dataset[column].apply(lambda x: 1 if str(x).upper() == 'TRUE' else (0 if str(x).upper() == 'FALSE' else None) if x != '' else None)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "tria_columns = ['region']\n",
    "\n",
    "# Define the mapping for 'region'\n",
    "region_mapping = {'Brussels': 1, 'Wallonie': 2, 'Flanders': 3, '': None}\n",
    "\n",
    "# Convert strings to integers and handle empty cells\n",
    "for column in tria_columns:\n",
    "    dataset[column] = dataset[column].map(region_mapping)\n",
    "    \n",
    "# Save the entire DataFrame to a CSV file\n",
    "csv_output_path = output_folder / 'recoded_dataset.csv'\n",
    "dataset.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(\"\\nDataFrame with 'furnished' column recoded:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "tria_columns = ['region']\n",
    "\n",
    "# Define the mapping for 'region'\n",
    "region_mapping = {'Brussels': 1, 'Wallonie': 2, 'Flanders': 3, '': None}\n",
    "\n",
    "# Convert strings to integers and handle empty cells\n",
    "for column in tria_columns:\n",
    "    dataset[column] = dataset[column].map(region_mapping)\n",
    "\n",
    "# Save the entire DataFrame to a CSV file\n",
    "csv_output_path = output_folder / 'recoded_dataset.csv'\n",
    "dataset.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"\\nDataFrame with 'region' column recoded to 1/2/3/None:\")\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values in the 'furnished' column before recoding\n",
    "print(\"Unique values in 'furnished' column before recoding:\")\n",
    "print(dataset['furnished'].unique())\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "binary_columns = ['furnished']\n",
    "\n",
    "# Convert 'TRUE'/'FALSE' strings to 1/0 integers and handle empty values\n",
    "for column in binary_columns:\n",
    "    dataset[column] = dataset[column].replace({'TRUE': 1, 'FALSE': 0, '': None, ' ': None})\n",
    "\n",
    "# Save the entire DataFrame to a CSV file\n",
    "csv_output_path = output_folder / 'recoded_dataset.csv'\n",
    "dataset.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# Print unique values in the 'furnished' column after recoding\n",
    "print(\"\\nDataFrame with 'furnished' column recoded:\")\n",
    "print(dataset['furnished'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the True False \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "binary_columns = ['furnished', 'terrace', 'garden', 'swimming_pool']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## working code with ID \n",
    "## FOR CODE TRIAL \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Function to scrape URLs\n",
    "def scrape_urls(page_num):\n",
    "    base_url = f\"https://www.immoweb.be/en/search/house-and-apartment/for-sale?countries=BE&page={page_num}&orderBy=relevance\"\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    urls = []\n",
    "    for elem in soup.find_all(\"a\", attrs={\"class\": \"card__title-link\"}):\n",
    "        urls.append(elem.get('href'))\n",
    "\n",
    "    # Save URLs to file - full_list.txt (local storage)\n",
    "    with open(\"full_list.txt\", \"a\") as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "    return urls\n",
    "\n",
    "def thread_scraping():\n",
    "    full_list_url = []\n",
    "    num_pages = 2\n",
    "\n",
    "    # Create a list to store threads\n",
    "    threads = []\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    # Create and start threads\n",
    "    for i in range(1, num_pages + 1):\n",
    "        t = threading.Thread(target=lambda: full_list_url.extend(scrape_urls(i)))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait for all threads to complete and then join\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end_time = time.time()  # Stop timer\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total URLs scraped:\", len(full_list_url))\n",
    "    print(\"Total time:\", execution_time, \"seconds\")\n",
    "    return full_list_url\n",
    "\n",
    "thread_scraping()\n",
    "\n",
    "\n",
    "def reporting(str, i):\n",
    "    \"\"\"Reports on scraping progress\"\"\"\n",
    "    sys.stdout.write(str + ' %d\\r' %i)\n",
    "    sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "# Initialize counter for the counter function\n",
    "counters = 1\n",
    "def counter():\n",
    "    \"\"\"Creates a global counter for use in list comprehension\"\"\"\n",
    "    global counters\n",
    "    if counters < 1:\n",
    "        counters = 1\n",
    "    else:\n",
    "        counters += 1\n",
    "    return\n",
    "\n",
    "def scrape_house(url):\n",
    "    \"\"\"Scrapes all the info from a house listing\"\"\"\n",
    "\n",
    "    # Get the house listing and make a soup\n",
    "    try:\n",
    "        house_page = requests.get(url)\n",
    "        house_page = BeautifulSoup(house_page.text, 'html.parser')\n",
    "    # Return an empty dictionary if we can't parse the URL\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    # Get the hidden info from the java script\n",
    "    try:\n",
    "        regex = r\"window.classified = (\\{.*\\})\" # Only captures what's between brackets\n",
    "        script = house_page.find('div',attrs={\"id\":\"main-container\"}).script.text\n",
    "        script = re.findall(regex, script)\n",
    "        script = json.loads(script[0])\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    final_dictionary = {}\n",
    "    # URL\n",
    "    try:\n",
    "        final_dictionary['url'] = url\n",
    "    except:\n",
    "        final_dictionary['url'] = 'UNKNOWN'\n",
    "    # URL adding IMMO ID\n",
    "    try:\n",
    "        final_dictionary['id'] = script['id']\n",
    "    except:\n",
    "        final_dictionary['id'] = 'UNKNOWN'\n",
    "    # Region\n",
    "    try:\n",
    "        final_dictionary['region'] = script['property']['location']['region']\n",
    "    except:\n",
    "        final_dictionary['region'] = 'UNKNOWN'\n",
    "    #  # Region\n",
    "    try:\n",
    "        final_dictionary['region'] = script['property']['location']['region']\n",
    "    except:\n",
    "        final_dictionary['region'] = 'UNKNOWN'\n",
    "    # Province\n",
    "    try:\n",
    "        final_dictionary['province'] = script['property']['location']['province']\n",
    "    except:\n",
    "        final_dictionary['province'] = 'UNKNOWN'\n",
    "    # Locality\n",
    "    try:\n",
    "        final_dictionary['locality'] = script['property']['location']['locality']\n",
    "    except:\n",
    "        final_dictionary['locality'] = 'UNKNOWN'\n",
    "    # ZIP Code\n",
    "    try:\n",
    "        final_dictionary['zip_code'] = script['property']['location']['postalCode']\n",
    "    except:\n",
    "        final_dictionary['zip_code'] = 'UNKNOWN'\n",
    "    # Longitude\n",
    "    try:\n",
    "        final_dictionary['Longitude'] = script['property']['location']['longitude']\n",
    "    except:\n",
    "        final_dictionary['Longitude'] = 'UNKNOWN'\n",
    "    # Latitude\n",
    "    try:\n",
    "        final_dictionary['Latitude'] = script['property']['location']['latitude']\n",
    "    except:\n",
    "        final_dictionary['Latitude'] = 'UNKNOWN'\n",
    "    # Type of property\n",
    "    try:\n",
    "        final_dictionary['property_type'] = script['property']['type']\n",
    "    except:\n",
    "        final_dictionary['property_type'] = 'UNKNOWN'\n",
    "    # Subtype of property\n",
    "    try:\n",
    "        final_dictionary['property_subtype'] = script['property']['subtype']\n",
    "    except:\n",
    "        final_dictionary['property_subtype'] = 'UNKNOWN'\n",
    "    # Price\n",
    "    try:\n",
    "        final_dictionary['price'] = script['price']['mainValue']\n",
    "    except:\n",
    "        final_dictionary['price'] = 'UNKNOWN'\n",
    "    # Number of rooms\n",
    "    try:\n",
    "        final_dictionary['number_rooms'] = script['property']['bedroomCount']\n",
    "    except:\n",
    "        final_dictionary['number_rooms'] = 'UNKNOWN'\n",
    "    # Living area\n",
    "    try:\n",
    "        final_dictionary['living_area'] = script['property']['netHabitableSurface']\n",
    "    except:\n",
    "        final_dictionary['living_area'] = 'UNKNOWN'\n",
    "    # Fully equipped kitchen (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['kitchen'] = script['property']['kitchen']['type']\n",
    "    except:\n",
    "        final_dictionary['kitchen'] = 0\n",
    "    # Furnished (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['furnished'] = script['transaction']['sale']['isFurnished']\n",
    "    except:\n",
    "        final_dictionary['furnished'] = 'UNKNOWN'\n",
    "    # Open fire (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['fireplace'] = script['property']['fireplaceCount']\n",
    "    except:\n",
    "        final_dictionary['fireplace'] = 0\n",
    "    # Terrace (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['terrace'] = script['property']['hasTerrace']\n",
    "    except:\n",
    "        final_dictionary['terrace'] = 0\n",
    "    # If yes: Area\n",
    "    try:\n",
    "        final_dictionary['terrace_area'] = script['property']['terraceSurface']\n",
    "    except:\n",
    "        final_dictionary['terrace_area'] = 0\n",
    "    # Garden\n",
    "    try:\n",
    "        final_dictionary['garden'] = script['property']['hasGarden']\n",
    "    except:\n",
    "        final_dictionary['garden'] = 0\n",
    "    # If yes: Area\n",
    "    try:\n",
    "        final_dictionary['garden_area'] = script['property']['gardenSurface']\n",
    "    except:\n",
    "        final_dictionary['garden_area'] = 0\n",
    "    # Surface of the land\n",
    "    try:\n",
    "        final_dictionary['surface_land'] = script['property']['land']['surface']\n",
    "    except:\n",
    "        final_dictionary['surface_land'] = \"UNKNOWN\"\n",
    "    # Number of facades\n",
    "    try:\n",
    "        final_dictionary['number_facades'] = script['property']['building']['facadeCount']\n",
    "    except:\n",
    "        final_dictionary['number_facades'] = \"UNKNOWN\"\n",
    "    # Swimming pool (Yes/No)\n",
    "    try:\n",
    "        final_dictionary['swimming_pool'] =  script['property']['hasSwimmingPool']\n",
    "    except:\n",
    "        final_dictionary['swimming_pool'] = 0\n",
    "    # State of the building (New, to be renovated, ...)\n",
    "    try:\n",
    "        final_dictionary['building_state'] = script['property']['building']['condition']\n",
    "    except:\n",
    "        final_dictionary['building_state'] = 'UNKNOWN'\n",
    "\n",
    "    return final_dictionary\n",
    "\n",
    "\n",
    "    return final_dictionary\n",
    "\n",
    "def create_dataframe():\n",
    "    \"\"\"Will scrape info from house pages and create a pandas DataFrame from the info we scrape\"\"\"\n",
    "    # Initialize list and fetch all URLs\n",
    "    houses_links = []\n",
    "    houses_links = thread_scraping()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Scraping individual pages...\")\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    # Scrape info from house pages concurrently\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [(executor.submit(scrape_house, url), counter(), reporting(\"Individual pages scraped:\", counters), time.sleep(.2)) for url in houses_links]\n",
    "        results =  [item[0].result() for item in futures]\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "    # Export our dataset to a csv\"\n",
    "    # Build path to file\n",
    "    # Select current working directory \n",
    "        cwd = Path.cwd()\n",
    "    df.to_csv(csv_path, index = True)\n",
    "\n",
    "    end_time = time.time()  # Stop timer\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(\"Scraping completed!                        \")\n",
    "    print(\"Total time spent scraping:\", execution_time, \"seconds\")\n",
    "    return df\n",
    "\n",
    "# Initialize counter for the counter function\n",
    "counters = 1\n",
    "\n",
    "# Build path to file\n",
    "# Selects current working directory\n",
    "cwd = Path.cwd()\n",
    "output_folder = (cwd / 'data_output').resolve() # Adjusted CSV file path and name\n",
    "csv_filename = \"house_apart_sale.csv\"\n",
    "csv_path = (output_folder / csv_filename).resolve()\n",
    "url_path = './full_list.txt'\n",
    "csv_path = (cwd / csv_path).resolve()\n",
    "url_path = (cwd / url_path).resolve()\n",
    "\n",
    "\n",
    "# Ensure the \"output\" folder exists\n",
    "output_folder = (cwd / 'data_output').resolve()\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset = create_dataframe()\n",
    "print(dataset)\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "column_names = dataset.columns\n",
    "print(column_names)\n",
    "\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "csv_file_path = './data_output/house_apart_sale.csv'\n",
    "\n",
    "# Read CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# List of columns to convert from yes/no strings to 1/0 integers\n",
    "binary_columns = ['furnished']\n",
    "\n",
    "# Convert yes/no strings to 1/0 integers and handle empty values\n",
    "for column in binary_columns:\n",
    "    df[column] = df[column].astype(str).str.upper().str.strip().map({'TRUE': 1, 'FALSE': 0, '': None})\n",
    "     \n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
