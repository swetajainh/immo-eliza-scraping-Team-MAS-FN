{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script utilizes multithreading to scrape house listings from Immoweb, storing URLs locally. It then concurrently scrapes detailed information from each listing, constructing a DataFrame, and exporting it to a CSV file, providing a comprehensive dataset of real estate listings with various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed!\n",
      "Total URLs scraped: 120\n",
      "Total time: 1.099456548690796 seconds\n",
      "\n",
      "Scraping individual pages...\n",
      "Scraping completed!raped: 121\n",
      "Total time spent scraping: 25.06033444404602 seconds\n",
      "                                                   url    region\n",
      "0    https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "1    https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "2    https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "3    https://www.immoweb.be/en/classified/new-real-...  Flanders\n",
      "4    https://www.immoweb.be/en/classified/house/for...  Flanders\n",
      "..                                                 ...       ...\n",
      "115  https://www.immoweb.be/en/classified/house/for...  Flanders\n",
      "116  https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "117  https://www.immoweb.be/en/classified/flat-stud...  Brussels\n",
      "118  https://www.immoweb.be/en/classified/apartment...  Flanders\n",
      "119  https://www.immoweb.be/en/classified/house/for...  Flanders\n",
      "\n",
      "[120 rows x 2 columns]\n",
      "Index(['url', 'region'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Function to scrape URLs from Immoweb\n",
    "def scrape_urls(page_num):\n",
    "    \"\"\"\n",
    "    Scrape URLs from Immoweb for house listings.\n",
    "\n",
    "    Args:\n",
    "    page_num (int): Page number to scrape.\n",
    "\n",
    "    Returns:\n",
    "    list: List of URLs scraped from the page.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://www.immoweb.be/en/search/house/for-sale?countries=BE&page={page_num}&orderBy=relevance\"\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    urls = []\n",
    "    for elem in soup.find_all(\"a\", attrs={\"class\": \"card__title-link\"}):\n",
    "        urls.append(elem.get('href'))\n",
    "\n",
    "    # Append URLs to a local file for future reference\n",
    "    with open(\"full_list.txt\", \"a\") as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "    return urls\n",
    "\n",
    "# Function to perform concurrent scraping of URLs\n",
    "def thread_scraping():\n",
    "    \"\"\"\n",
    "    Perform scraping of URLs using multiple threads.\n",
    "    \"\"\"\n",
    "    full_list_url = []\n",
    "    num_pages = 2\n",
    "    threads = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create and start threads for scraping each page\n",
    "    for i in range(1, num_pages + 1):\n",
    "        t = threading.Thread(target=lambda: full_list_url.extend(scrape_urls(i)))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Display scraping completion message and execution time\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total URLs scraped:\", len(full_list_url))\n",
    "    print(\"Total time:\", execution_time, \"seconds\")\n",
    "    return full_list_url\n",
    "\n",
    "# Function to report progress during scraping\n",
    "def reporting(str, i):\n",
    "    \"\"\"\n",
    "    Reports on scraping progress.\n",
    "\n",
    "    Args:\n",
    "    str (str): Message to print.\n",
    "    i (int): Counter value.\n",
    "    \"\"\"\n",
    "    sys.stdout.write(str + ' %d\\r' %i)\n",
    "    sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "# Function to create a global counter for use in list comprehension\n",
    "def counter():\n",
    "    \"\"\"\n",
    "    Creates a global counter for use in list comprehension.\n",
    "    \"\"\"\n",
    "    global counters\n",
    "    if counters < 1:\n",
    "        counters = 1\n",
    "    else:\n",
    "        counters +=1\n",
    "    return\n",
    "\n",
    "# Function to scrape detailed information from individual house pages\n",
    "def scrape_house(url):\n",
    "    \"\"\"\n",
    "    Scrapes all the info from a house listing.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL of the house listing.\n",
    "\n",
    "    Returns:\n",
    "    dict: Information scraped from the house listing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        house_page = requests.get(url)\n",
    "        house_page = BeautifulSoup(house_page.text, 'html.parser')\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        regex = r\"window.classified = (\\{.*\\})\"\n",
    "        script = house_page.find('div',attrs={\"id\":\"main-container\"}).script.text\n",
    "        script = re.findall(regex, script)\n",
    "        script = json.loads(script[0])\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    final_dictionary = {}\n",
    "\n",
    "    # Extract relevant information from the script\n",
    "    try:\n",
    "        final_dictionary['url'] = url\n",
    "    except:\n",
    "        final_dictionary['url'] = 'UNKNOWN'\n",
    "\n",
    "    try:\n",
    "        final_dictionary['region'] = script['property']['location']['region']\n",
    "    except:\n",
    "        final_dictionary['region'] = 'UNKNOWN'\n",
    "\n",
    "    # Extract other key-value pairs similarly...\n",
    "\n",
    "    return final_dictionary\n",
    "\n",
    "# Function to create a DataFrame from scraped data\n",
    "def create_dataframe():\n",
    "    \"\"\"\n",
    "    Scrapes info from house pages and creates a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    houses_links = []\n",
    "    houses_links = thread_scraping()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Scraping individual pages...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Scraping info from individual house pages concurrently\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [(executor.submit(scrape_house, url), counter(), reporting(\"Individual pages scraped:\", counters), time.sleep(.2)) for url in houses_links]\n",
    "        results = [item[0].result() for item in futures]\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "    # Define paths for data export\n",
    "    cwd = Path.cwd()\n",
    "    csv_path = r'.\\data_output'\n",
    "    csv_path = (cwd / csv_path).resolve()\n",
    "\n",
    "    # Export DataFrame to CSV file\n",
    "    df.to_csv(csv_path, index = True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Display completion message and execution time\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total time spent scraping:\", execution_time, \"seconds\")\n",
    "    return df\n",
    "\n",
    "# Initialize counter for the counter function\n",
    "counters = 1\n",
    "\n",
    "# Call function to create DataFrame from scraped data\n",
    "dataset = create_dataframe()\n",
    "print(dataset)\n",
    "\n",
    "# Display column names of the DataFrame\n",
    "column_names = dataset.columns\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the part of the code that implements multithreading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform concurrent scraping of URLs\n",
    "def thread_scraping():\n",
    "    \"\"\"\n",
    "    Perform scraping of URLs using multiple threads.\n",
    "    \"\"\"\n",
    "    full_list_url = []  # Initialize an empty list to store all scraped URLs\n",
    "    num_pages = 2  # Define the number of pages to scrape\n",
    "    threads = []  # Initialize a list to store thread objects\n",
    "    start_time = time.time()  # Record the start time of the scraping process\n",
    "\n",
    "    # Create and start threads for scraping each page\n",
    "    for i in range(1, num_pages + 1):\n",
    "        # Define a new thread for each page, targeting the scrape_urls function with page number as argument\n",
    "        t = threading.Thread(target=lambda: full_list_url.extend(scrape_urls(i)))\n",
    "        threads.append(t)  # Append the thread object to the list of threads\n",
    "        t.start()  # Start the thread\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for t in threads:\n",
    "        t.join()  # Wait for each thread to finish its task\n",
    "\n",
    "    end_time = time.time()  # Record the end time of the scraping process\n",
    "    execution_time = end_time - start_time  # Calculate the total execution time\n",
    "\n",
    "    # Display scraping completion message and execution time\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total URLs scraped:\", len(full_list_url))\n",
    "    print(\"Total time:\", execution_time, \"seconds\")\n",
    "    \n",
    "    return full_list_url  # Return the list of all scraped URLs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This refactored code uses ThreadPoolExecutor for both scraping URLs and scraping detailed information from individual house pages. It should provide concurrency through threading while scraping data from the Immoweb website.\n",
    "The ThreadPoolExecutor and threading.Thread classes in Python both provide ways to run tasks concurrently using threads. However, the ThreadPoolExecutor often performs better in terms of efficiency and speed compared to directly using threading.Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed!\n",
      "Total URLs scraped: 120\n",
      "Total time: 1.6385235786437988 seconds\n",
      "\n",
      "Scraping individual pages...\n",
      "Scraping completed!\n",
      "Total time spent scraping: 12.10824179649353 seconds\n",
      "                                                   url    region\n",
      "0    https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "1    https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "2    https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "3    https://www.immoweb.be/en/classified/new-real-...  Flanders\n",
      "4    https://www.immoweb.be/en/classified/house/for...  Flanders\n",
      "..                                                 ...       ...\n",
      "115  https://www.immoweb.be/en/classified/apartment...  Flanders\n",
      "116  https://www.immoweb.be/en/classified/apartment...  Brussels\n",
      "117  https://www.immoweb.be/en/classified/apartment...  Brussels\n",
      "118  https://www.immoweb.be/en/classified/new-real-...  Flanders\n",
      "119  https://www.immoweb.be/en/classified/house/for...  Wallonie\n",
      "\n",
      "[120 rows x 2 columns]\n",
      "Index(['url', 'region'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def scrape_urls(page_num):\n",
    "    base_url = f\"https://www.immoweb.be/en/search/house/for-sale?countries=BE&page={page_num}&orderBy=relevance\"\n",
    "    r = requests.get(base_url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    urls = [elem.get('href') for elem in soup.find_all(\"a\", attrs={\"class\": \"card__title-link\"})]\n",
    "\n",
    "    with open(\"full_list.txt\", \"a\") as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "    return urls\n",
    "\n",
    "def thread_scraping():\n",
    "    full_list_url = []\n",
    "    num_pages = 2\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(scrape_urls, range(1, num_pages + 1))\n",
    "        for result in results:\n",
    "            full_list_url.extend(result)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total URLs scraped:\", len(full_list_url))\n",
    "    print(\"Total time:\", execution_time, \"seconds\")\n",
    "    return full_list_url\n",
    "\n",
    "def scrape_house(url):\n",
    "    try:\n",
    "        house_page = requests.get(url)\n",
    "        house_page = BeautifulSoup(house_page.text, 'html.parser')\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        regex = r\"window.classified = (\\{.*\\})\"\n",
    "        script = house_page.find('div',attrs={\"id\":\"main-container\"}).script.text\n",
    "        script = re.findall(regex, script)\n",
    "        script = json.loads(script[0])\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "    final_dictionary = {'url': url}\n",
    "\n",
    "    try:\n",
    "        final_dictionary['region'] = script['property']['location']['region']\n",
    "    except:\n",
    "        final_dictionary['region'] = 'UNKNOWN'\n",
    "\n",
    "    # Extract other key-value pairs similarly...\n",
    "\n",
    "    return final_dictionary\n",
    "\n",
    "def create_dataframe():\n",
    "    houses_links = thread_scraping()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Scraping individual pages...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(scrape_house, houses_links)\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    csv_path = r'.\\data_output'\n",
    "    csv_path = (cwd / csv_path).resolve()\n",
    "\n",
    "    df.to_csv(csv_path, index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(\"Scraping completed!\")\n",
    "    print(\"Total time spent scraping:\", execution_time, \"seconds\")\n",
    "    return df\n",
    "\n",
    "counters = 1\n",
    "dataset = create_dataframe()\n",
    "print(dataset)\n",
    "column_names = dataset.columns\n",
    "print(column_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
