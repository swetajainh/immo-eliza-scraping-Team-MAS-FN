{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from scraper import scrape_urls\n",
    "from scraper import thread_scraping\n",
    "from scraper import scrape_house\n",
    "from scraper import counter\n",
    "from scraper import create_dataframe\n",
    "from scraper import reporting\n",
    "import csv\n",
    "\n",
    "# Initialize counter for the counter function\n",
    "counters = 1\n",
    "\n",
    "def main():\n",
    "    # Select current working directory\n",
    "    cwd = Path.cwd()\n",
    "\n",
    "    # Define file paths\n",
    "    csv_path = cwd / 'house_apart_sale' \n",
    "    url_path = cwd / 'full_list.txt'\n",
    "\n",
    "    # Create DataFrame from scraped data\n",
    "    dataset = create_dataframe()\n",
    "\n",
    "    # Write DataFrame to CSV\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=dataset.columns)\n",
    "        writer.writeheader()\n",
    "        for index, row in dataset.iterrows():\n",
    "            writer.writerow(row.to_dict())\n",
    "\n",
    "    # Read CSV back into DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(df.head())\n",
    "\n",
    "# Call the main function\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
